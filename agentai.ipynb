{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nico-lina/challenge_aibot/blob/main/agentai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss8alZF2SVkD"
      },
      "source": [
        "#librerie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97lBLD7uRlKM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8627d44-613d-401a-8afa-eab91a91359a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.12)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: mistralai in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.5)\n",
            "Requirement already satisfied: fastapi==0.115.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.115.9)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.3)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.14.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.34.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.34.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.55b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.34.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.72.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (32.0.1)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.18)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.24.0)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.115.9->chromadb) (0.45.3)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.12)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: eval-type-backport>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from mistralai) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.15.3)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.59 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.63)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.25)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.44)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.25.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.34.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.55b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.55b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.55b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.55b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.55b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.55b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.55b0)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.55b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.55b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.22.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install datasets chromadb gradio mistralai transformers sentence_transformers langchain_community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNuZVn2CSuTh"
      },
      "source": [
        "#Attach to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKI3kdxAStu3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "sys.path.append('/content/drive/MyDrive/AgentAI_Semantic_Similarity/data')\n",
        "sys.path.append('/content/drive/MyDrive/AgentAI_Semantic_Similarity/backend')\n",
        "sys.path.append('/content/drive/MyDrive/AgentAI_Semantic_Similarity/frontend')\n",
        "sys.path.append('/content/drive/MyDrive/AgentAI_Semantic_Similarity/model')\n",
        "sys.path.append('/content/drive/MyDrive/AgentAI_Semantic_Similarity/VDB')\n",
        "os.chdir('/content/drive/MyDrive/AgentAI_Semantic_Similarity/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1u0AGcrnAgnC",
        "outputId": "c157d009-f902-41da-f8dc-963f087ebd29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRlgQtyzTcXN"
      },
      "source": [
        "#Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiwCZ25NTgyq"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import requests\n",
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import userdata\n",
        "from collections import defaultdict\n",
        "\n",
        "def download_scryfall_bulk_data():\n",
        "    \"\"\"\n",
        "    Recupera il 'download_uri' dall'API Scryfall per i dati di massa delle carte predefinite\n",
        "    e scarica il file JSON associato nella directory specificata.\n",
        "\n",
        "    \"\"\"\n",
        "    api_url = \"https://api.scryfall.com/bulk-data/oracle_cards\"\n",
        "    full_output_path = userdata.get('card_path')\n",
        "    try:\n",
        "        response_api = requests.get(api_url)\n",
        "        response_api.raise_for_status() # Solleva un'eccezione se ci sono codici d'errore\n",
        "        data = response_api.json()\n",
        "\n",
        "        download_uri = data.get(\"download_uri\")\n",
        "        if not download_uri:\n",
        "            print(\"La chiave 'download_uri' non è stata trovata nell'oggetto JSON.\")\n",
        "            return\n",
        "        response_download = requests.get(download_uri, stream=True)\n",
        "        response_download.raise_for_status()\n",
        "        with open(full_output_path, 'wb') as f:\n",
        "            for chunk in response_download.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Errore durante la richiesta API o il download: {e}\")\n",
        "    except ValueError as e:\n",
        "        print(f\"Errore durante il parsing del JSON: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Si è verificato un errore inatteso: {e}\")\n",
        "\n",
        "def select_columns():\n",
        "  \"\"\"\n",
        "  Seleziona le colonne che dovranno essere filtrate e salvate in un nuovo file JSON.\n",
        "  Successivamente subiranno una fase di data preparetion\n",
        "  \"\"\"\n",
        "  file_path=userdata.get('card_path')\n",
        "  with open(file_path, 'r', encoding = 'utf-8') as f:\n",
        "    data = json.load(f)\n",
        "  selected_columns = ['name','cmc', 'type_line', 'oracle_text', 'power', 'toughness', 'color_identity', 'keywords','legalities','image_uris']\n",
        "  df = pd.DataFrame(data)[selected_columns]\n",
        "\n",
        "  df.to_json(userdata.get('filtered_card'), orient = 'records')\n",
        "\n",
        "\n",
        "def data_preparation():\n",
        "  with open(userdata.get('filtered_card'), 'r', encoding = 'utf-8') as f:\n",
        "    data = json.load(f)\n",
        "  name_card_set = set()\n",
        "  sub_type_set = set()\n",
        "  type_set = set()\n",
        "  keyword_set = set()\n",
        "  type_set.add(\"\")\n",
        "  sub_type_set.add(\"\")\n",
        "  correct_db = []\n",
        "  separator = \"—\"\n",
        "  legalita_mapping = {\n",
        "      \"standard\": 0,      # 2^0 = 1\n",
        "      \"future\": 1,        # 2^1 = 2\n",
        "      \"historic\": 2,      # 2^2 = 4\n",
        "      \"timeless\": 3,      # 2^3 = 8\n",
        "      \"gladiator\": 4,     # 2^4 = 16\n",
        "      \"pioneer\": 5,       # 2^5 = 32\n",
        "      \"explorer\": 6,      # 2^6 = 64\n",
        "      \"modern\": 7,        # 2^7 = 128\n",
        "      \"legacy\": 8,        # 2^8 = 256\n",
        "      \"pauper\": 9,        # 2^9 = 512\n",
        "      \"vintage\": 10,       # 2^10 = 1024\n",
        "      \"penny\": 11,        # 2^11 = 2048\n",
        "      \"commander\": 12,     # 2^12 = 4096\n",
        "      \"oathbreaker\": 13,   # 2^13 = 8192\n",
        "      \"standardbrawl\": 14, # 2^14 = 16384\n",
        "      \"brawl\": 15,        # 2^15 = 32768\n",
        "      \"alchemy\": 16,      # 2^16 = 65536\n",
        "      \"paupercommander\": 17,# 2^17 = 131072\n",
        "      \"duel\": 18,         # 2^18 = 262144\n",
        "      \"oldschool\": 19,     # 2^19 = 524288\n",
        "      \"premodern\": 20,     # 2^20 = 1048576\n",
        "      \"predh\": 21          # 2^21 = 2097152\n",
        "  }\n",
        "\n",
        "  keyword_counts = defaultdict(int)\n",
        "  dont_swap=['Awaken','Partner','Channel','Cipher','Conspire','Cycling','Dash','Dredge','Embalm','Encore','Entwine','Escape','Eternalize','Evoke ','Flashback','Forecast','Foretell','Fuse','Grandeur','Hideaway','Jump-start','Overload','Plot','Rebound','Recover','Reinforce','Retrace','Unearth','Venture into the Dungeon' ]\n",
        "  for card in data:\n",
        "      type_line = card.get('type_line', '')\n",
        "      type_c = ''\n",
        "      sub_type = ''\n",
        "      legendary = 0\n",
        "      bitmask = 0\n",
        "      # Seleziono solo le carte valide\n",
        "      if \"Token\" not in type_line and \"Emblem\" not in type_line and \"Summon\" not in type_line and card.get('name', '') not in name_card_set and '//' not in card.get('name', ''):\n",
        "          name_card_set.add(card.get('name', ''))\n",
        "\n",
        "          if separator in type_line:\n",
        "              segments = type_line.split(separator, 1)\n",
        "              type_c = segments[0].strip()  # Prelevo il tipo\n",
        "              sub_type = segments[1].strip()  # Prelevo il sottotipo\n",
        "              # Salvo tutti i tipi e sottotipi e tutte le loro combinazioni possibili\n",
        "              sub_type_set.add(sub_type)\n",
        "          else:\n",
        "              type_c= type_line\n",
        "              sub_type = 'tappo'\n",
        "\n",
        "          if \"Legendary\" in type_c:  # Gestisco il leggendario\n",
        "              legendary = 1\n",
        "              type_c = type_c.replace(\"Legendary\", \"\").strip()\n",
        "          type_set.add(type_c)\n",
        "          if card.get('color_identity', '') == []:  # Sistemo il campo per il colorless\n",
        "              card['color_identity'] = ['CL']\n",
        "\n",
        "          if card.get('keywords'):  # Salvo tutte le keyword esistenti\n",
        "              for keyword in card.get('keywords'):\n",
        "                  keyword_set.add(keyword)\n",
        "\n",
        "\n",
        "          row = card.get('oracle_text', '').split('\\n')\n",
        "          # Se esistono più righe nella carta e corrisponde ad uno dei seguenti tipi e\n",
        "          #non sono presenti le parole chiave in dont_swap sposto la prima sezione infondo al testo\n",
        "          if (len(row) > 1 and\n",
        "             (\"Creature\" in type_c or \"Sorcery\" in type_c or \"Instant\" in type_c) and\n",
        "             all(word not in card['oracle_text'] for word in dont_swap)):\n",
        "              card['oracle_text'] = '\\n'.join(row[1:] + [row[0]])\n",
        "\n",
        "          if \"Enchantment\" in type_c and \"God\" in sub_type:\n",
        "              card['oracle_text'] = '\\n'.join(row[1:] + [row[0]])\n",
        "          for format, leg in card.get(\"legalities\").items():\n",
        "            if leg == \"legal\" or leg == \"restricted\":\n",
        "              bitmask |= (1 << legalita_mapping[format])\n",
        "          for uri_type,uri in card.get(\"image_uris\").items():\n",
        "            if uri_type == \"border_crop\":\n",
        "              card[\"image_uris\"]=uri\n",
        "          card['legalities']=bitmask\n",
        "          # Aggiungi i nuovi campi \"Legendary\", \"Type\", \"SubType\" alla carta\n",
        "          card['Legendary'] = legendary\n",
        "          card['Type'] = type_c\n",
        "          card['SubType'] = sub_type\n",
        "\n",
        "          correct_db.append(card)\n",
        "\n",
        "  with open('/content/drive/MyDrive/AgentAI_Semantic_Similarity/data/filtered_cards.json', 'w', encoding = 'utf-8') as f:\n",
        "      json.dump(correct_db, f, indent= 4)\n",
        "  with open('/content/drive/MyDrive/AgentAI_Semantic_Similarity/data/name_cards.json', 'w', encoding = 'utf-8') as f:\n",
        "      json.dump(list(name_card_set), f, indent= 4)\n",
        "  with open('/content/drive/MyDrive/AgentAI_Semantic_Similarity/data/type.json', 'w', encoding = 'utf-8') as f:\n",
        "      json.dump(list(type_set), f, indent= 4)\n",
        "  with open('/content/drive/MyDrive/AgentAI_Semantic_Similarity/data/sub_type.json', 'w', encoding = 'utf-8') as f:\n",
        "      json.dump(list(sub_type_set), f, indent= 4)\n",
        "  with open('/content/drive/MyDrive/AgentAI_Semantic_Similarity/data/keyword.json', 'w', encoding = 'utf-8') as f:\n",
        "      json.dump(list(keyword_set), f, indent= 4)\n",
        "\n",
        "\n",
        "download_scryfall_bulk_data()\n",
        "select_columns()\n",
        "data_preparation()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnWEeh12sxFB"
      },
      "source": [
        "#Create VectorDatabase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tmojSqt9hZl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "80299d13-7bb6-409b-f101-193195379b9a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import shutil\\nimport os\\n\\n# Define the path to your ChromaDB persistent client data\\nchroma_db_path = \"/content/drive/MyDrive/AgentAI_Semantic_Similarity/data/VDB_mod_2\"\\n\\n# Check if the directory exists before attempting to delete it\\nif os.path.exists(chroma_db_path):\\n    try:\\n        shutil.rmtree(chroma_db_path)\\n        print(f\"Directory \\'{chroma_db_path}\\' and its contents deleted successfully.\")\\n    except Exception as e:\\n        print(f\"Error deleting directory \\'{chroma_db_path}\\': {e}\")\\nelse:\\n    print(f\"Directory \\'{chroma_db_path}\\' does not exist. No need to delete.\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "'''from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# 1. Load the model and tokenizer\n",
        "model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# 2. Define the output path\n",
        "output_path = '/content/drive/MyDrive/AgentAI_Semantic_Similarity/model_output'\n",
        "\n",
        "# 3. Save the model and tokenizer\n",
        "#tokenizer.save_pretrained(output_path)\n",
        "#model.save_pretrained(output_path)\n",
        "\n",
        "print(f\"Model '{model_name}' saved to '{output_path}'\")'''\n",
        "'''import shutil\n",
        "import os\n",
        "\n",
        "# Define the path to your ChromaDB persistent client data\n",
        "chroma_db_path = \"/content/drive/MyDrive/AgentAI_Semantic_Similarity/data/VDB_mod_2\"\n",
        "\n",
        "# Check if the directory exists before attempting to delete it\n",
        "if os.path.exists(chroma_db_path):\n",
        "    try:\n",
        "        shutil.rmtree(chroma_db_path)\n",
        "        print(f\"Directory '{chroma_db_path}' and its contents deleted successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error deleting directory '{chroma_db_path}': {e}\")\n",
        "else:\n",
        "    print(f\"Directory '{chroma_db_path}' does not exist. No need to delete.\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8b0hhLes3Fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e914269c-8be9-46ae-8d1e-46e103503814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utilizzando il dispositivo: cuda\n",
            "Collezione 'magic_vdb_mod_2' cancellata con successo (se esisteva).\n",
            "Collezione 'magic_vdb' creata/ottenuta con i parametri HNSW aggiornati.\n",
            "Elaborato e aggiunto il batch 1\n",
            "Elaborato e aggiunto il batch 2\n",
            "Elaborato e aggiunto il batch 3\n",
            "Elaborato e aggiunto il batch 4\n",
            "Elaborato e aggiunto il batch 5\n",
            "Elaborato e aggiunto il batch 6\n",
            "Elaborato e aggiunto il batch 7\n",
            "Elaborato e aggiunto il batch 8\n",
            "Elaborato e aggiunto il batch 9\n",
            "Elaborato e aggiunto il batch 10\n",
            "Elaborato e aggiunto il batch 11\n",
            "Elaborato e aggiunto il batch 12\n",
            "Elaborato e aggiunto il batch 13\n",
            "Elaborato e aggiunto il batch 14\n",
            "Elaborato e aggiunto il batch 15\n",
            "Elaborato e aggiunto il batch 16\n",
            "Elaborato e aggiunto il batch 17\n",
            "Elaborato e aggiunto il batch 18\n",
            "Elaborato e aggiunto il batch 19\n",
            "Elaborato e aggiunto il batch 20\n",
            "Elaborato e aggiunto il batch 21\n",
            "Elaborato e aggiunto il batch 22\n",
            "Elaborato e aggiunto il batch 23\n",
            "Elaborato e aggiunto il batch 24\n",
            "Elaborato e aggiunto il batch 25\n",
            "Elaborato e aggiunto il batch 26\n",
            "Elaborato e aggiunto il batch 27\n",
            "Elaborato e aggiunto il batch 28\n",
            "Elaborato e aggiunto il batch 29\n",
            "Elaborato e aggiunto il batch 30\n",
            "Elaborato e aggiunto il batch 31\n",
            "Elaborato e aggiunto il batch 32\n",
            "Elaborato e aggiunto il batch 33\n",
            "Elaborato e aggiunto il batch 34\n",
            "Elaborato e aggiunto il batch 35\n",
            "Elaborato e aggiunto il batch 36\n",
            "Elaborato e aggiunto il batch 37\n",
            "Elaborato e aggiunto il batch 38\n",
            "Elaborato e aggiunto il batch 39\n",
            "Elaborato e aggiunto il batch 40\n",
            "Elaborato e aggiunto il batch 41\n",
            "Elaborato e aggiunto il batch 42\n",
            "Elaborato e aggiunto il batch 43\n",
            "Elaborato e aggiunto il batch 44\n",
            "Elaborato e aggiunto il batch 45\n",
            "Elaborato e aggiunto il batch 46\n",
            "Elaborato e aggiunto il batch 47\n",
            "Elaborato e aggiunto il batch 48\n",
            "Elaborato e aggiunto il batch 49\n",
            "Elaborato e aggiunto il batch 50\n",
            "Elaborato e aggiunto il batch 51\n",
            "Elaborato e aggiunto il batch 52\n",
            "Elaborato e aggiunto il batch 53\n",
            "Elaborato e aggiunto il batch 54\n",
            "Elaborato e aggiunto il batch 55\n",
            "Elaborato e aggiunto il batch 56\n",
            "Elaborato e aggiunto il batch 57\n",
            "Elaborato e aggiunto il batch 58\n",
            "Elaborato e aggiunto il batch 59\n",
            "Elaborato e aggiunto il batch 60\n",
            "Elaborato e aggiunto il batch 61\n",
            "Elaborato e aggiunto il batch 62\n",
            "Elaborato e aggiunto il batch 63\n",
            "Elaborato e aggiunto il batch 64\n",
            "Elaborato e aggiunto il batch 65\n",
            "Elaborato e aggiunto il batch 66\n",
            "Elaborato e aggiunto il batch 67\n",
            "Elaborato e aggiunto il batch 68\n",
            "Elaborato e aggiunto il batch 69\n",
            "Elaborato e aggiunto il batch 70\n",
            "Elaborato e aggiunto il batch 71\n",
            "Elaborato e aggiunto il batch 72\n",
            "Elaborato e aggiunto il batch 73\n",
            "Elaborato e aggiunto il batch 74\n",
            "Elaborato e aggiunto il batch 75\n",
            "Elaborato e aggiunto il batch 76\n",
            "Elaborato e aggiunto il batch 77\n",
            "Elaborato e aggiunto il batch 78\n",
            "Elaborato e aggiunto il batch 79\n",
            "Elaborato e inserito un totale di 31397 carte in Chroma\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import chromadb\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from chromadb import EmbeddingFunction, Documents, Embeddings\n",
        "\n",
        "# Percorso al modello Sentence-BERT fine-tunato\n",
        "model_path = \"/content/drive/MyDrive/AgentAI_Semantic_Similarity/model_output\"\n",
        "#model_path = \"/content/drive/MyDrive/AgentAI_Semantic_Similarity/model_mod_1\"\n",
        "# Verifica se la GPU è disponibile\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Utilizzando il dispositivo: {device}\")\n",
        "\n",
        "# Definizione della funzione di embedding con SentenceTransformer\n",
        "class MyEmbeddingFunction(EmbeddingFunction):\n",
        "    def __init__(self, model_path: str, device: str):\n",
        "        self.model = SentenceTransformer(model_path, device=device)\n",
        "\n",
        "    def __call__(self, input: Documents) -> Embeddings:\n",
        "        embeddings = self.model.encode(input, convert_to_numpy=True, show_progress_bar=False)\n",
        "        return embeddings.tolist()\n",
        "\n",
        "# Crea l'istanza della funzione di embedding\n",
        "embedding_function = MyEmbeddingFunction(model_path, device)\n",
        "\n",
        "# Inizializza ChromaDB\n",
        "client = chromadb.PersistentClient(path=\"/content/drive/MyDrive/AgentAI_Semantic_Similarity/data/VDB_mod_2\")\n",
        "\n",
        "\n",
        "################################################################################################\n",
        "collection_name = \"magic_vdb_mod_2\"\n",
        "try:\n",
        "    client.delete_collection(name=collection_name)\n",
        "    print(f\"Collezione '{collection_name}' cancellata con successo (se esisteva).\")\n",
        "except Exception as e:\n",
        "    print(f\"Impossibile cancellare la collezione '{collection_name}' (potrebbe non esistere): {e}\")\n",
        "###############################################################################################\n",
        "# Crea la nuova collezione con i parametri HNSW aggiornati\n",
        "collection = client.get_or_create_collection(\n",
        "    name=\"magic_vdb_mod_2\",\n",
        "    embedding_function=embedding_function,\n",
        "    configuration={\n",
        "        \"hnsw\":{\"space\": \"cosine\",\n",
        "                \"ef_construction\": 4000,\n",
        "                \"ef_search\": 500,\n",
        "                 \"max_neighbors\": 32}\n",
        "    }\n",
        ")\n",
        "print(\"Collezione 'magic_vdb' creata/ottenuta con i parametri HNSW aggiornati.\")\n",
        "\n",
        "\n",
        "# Carica le carte filtrate\n",
        "with open('/content/drive/MyDrive/AgentAI_Semantic_Similarity/data/filtered_cards.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "selected_columns = ['name', 'cmc', 'type_line', 'power', 'toughness', 'color_identity', 'keywords', 'legalities', 'image_uris', 'Legendary', 'Type', 'SubType']\n",
        "\n",
        "# Funzione per elaborare un batch di carte\n",
        "def process_batch(batch, start_id):\n",
        "    documents = []\n",
        "    metadatas = []\n",
        "    ids = []\n",
        "    for i, obj in enumerate(batch):\n",
        "        oracle_text = obj.get(\"oracle_text\", \"\")\n",
        "        metadata_entry = {}\n",
        "        for key in selected_columns:\n",
        "            value = obj.get(key)\n",
        "            if value is None:\n",
        "                metadata_entry[key] = \"\"\n",
        "            elif isinstance(value, list):\n",
        "                metadata_entry[key] = \", \".join(map(str, value))\n",
        "            elif isinstance(value, dict):\n",
        "                metadata_entry[key] = json.dumps(value)\n",
        "            else:\n",
        "                metadata_entry[key] = value\n",
        "\n",
        "        card_id = f\"card_{start_id + i}\"\n",
        "        metadata_entry[\"id\"] = card_id\n",
        "\n",
        "        documents.append(oracle_text)\n",
        "        metadatas.append(metadata_entry)\n",
        "        ids.append(card_id)\n",
        "\n",
        "    # Aggiunta delle carte a ChromaDB\n",
        "    collection.add(\n",
        "        documents=documents,\n",
        "        metadatas=metadatas,\n",
        "        ids=ids\n",
        "    )\n",
        "\n",
        "# Elaborazione delle carte in batch\n",
        "batch_size = 400\n",
        "current_id = 0\n",
        "for i in range(0, len(data), batch_size):\n",
        "    batch = data[i:i + batch_size]\n",
        "    process_batch(batch, current_id)\n",
        "    current_id += len(batch)\n",
        "    print(f\"Elaborato e aggiunto il batch {i // batch_size + 1}\")\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"Elaborato e inserito un totale di {len(data)} carte in Chroma\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uI56WvLNEXxk"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Percorso al modello Sentence-BERT fine-tunato\n",
        "model_path = \"/content/drive/MyDrive/AgentAI_Semantic_Similarity/model_output\"\n",
        "# Percorso al file JSON di input\n",
        "input_json_path = '/content/drive/MyDrive/AgentAI_Semantic_Similarity/data/filtered_cards.json'\n",
        "# Percorso al file JSON di output con gli embedding\n",
        "output_json_path = '/content/drive/MyDrive/AgentAI_Semantic_Similarity/data/filtered_cards_with_embeddings.json'\n",
        "\n",
        "# Verifica se la GPU è disponibile\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Utilizzando il dispositivo: {device}\")\n",
        "\n",
        "# Carica il modello SentenceTransformer\n",
        "try:\n",
        "    model = SentenceTransformer(model_path, device=device)\n",
        "    print(f\"Modello caricato da '{model_path}' sul dispositivo '{device}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Errore durante il caricamento del modello: {e}\")\n",
        "    print(\"Assicurati che il percorso del modello sia corretto e che il modello sia stato salvato correttamente.\")\n",
        "    exit() # Esci se il modello non può essere caricato\n",
        "\n",
        "# Carica le carte filtrate dal file JSON\n",
        "try:\n",
        "    with open(input_json_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    print(f\"Caricate {len(data)} carte da '{input_json_path}'.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Errore: Il file '{input_json_path}' non è stato trovato.\")\n",
        "    exit()\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"Errore: Impossibile decodificare il file JSON '{input_json_path}'. Controlla la sua formattazione.\")\n",
        "    exit()\n",
        "\n",
        "# Contatore per le carte elaborate\n",
        "processed_cards_count = 0\n",
        "\n",
        "# Itera su ogni carta per generare e aggiungere l'embedding\n",
        "print(\"Generazione degli embedding e aggiunta al file JSON...\")\n",
        "for i, card in enumerate(data):\n",
        "    oracle_text = card.get(\"oracle_text\", \"\")\n",
        "\n",
        "    # Genera l'embedding per il testo oracle\n",
        "    # L'embedding viene convertito in una lista Python per essere salvato nel JSON\n",
        "    try:\n",
        "        embedding = model.encode(oracle_text, convert_to_numpy=True, show_progress_bar=False)\n",
        "        card['embedding'] = embedding.tolist()\n",
        "        processed_cards_count += 1\n",
        "        if (i + 1) % 100 == 0: # Stampa un aggiornamento ogni 100 carte\n",
        "            print(f\"Elaborate {i + 1} carte...\")\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante la generazione dell'embedding per la carta {card.get('name', 'Sconosciuta')}: {e}\")\n",
        "        card['embedding'] = None # Imposta l'embedding a None in caso di errore\n",
        "\n",
        "    # Libera la memoria GPU dopo ogni embedding (buona pratica per batch più piccoli o iterazioni)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"Generati e aggiunti gli embedding per {processed_cards_count} carte.\")\n",
        "\n",
        "# Salva i dati aggiornati in un nuovo file JSON\n",
        "try:\n",
        "    with open(input_json_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Dati con gli embedding salvati con successo in '{input_json_path}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Errore durante il salvataggio del file JSON: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAaW1f4VB7cT"
      },
      "source": [
        "#GUI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R6Yxs9v6B9hf",
        "outputId": "1d8b611f-b9ba-49e3-b276-4ff10bbe9b35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/AgentAI_Semantic_Similarity/frontend/agentetool.py:15: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(\n",
            "/content/drive/MyDrive/AgentAI_Semantic_Similarity/frontend/agentetool.py:93: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
            "/content/drive/MyDrive/AgentAI_Semantic_Similarity/frontend/agentetool.py:96: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
            "  agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools)\n",
            "/content/drive/MyDrive/AgentAI_Semantic_Similarity/frontend/agentetool.py:99: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(\n",
            "/content/drive/MyDrive/AgentAI_Semantic_Similarity/frontend/UI_gradio.py:115: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(label=\"AI Chat\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://eb1868db6fc611b809.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://eb1868db6fc611b809.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/AgentAI_Semantic_Similarity/frontend/agentetool.py:115: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = agent_executor.run(user_input)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: The user wants a card that, when it enters the battlefield, allows them to draw two cards. This is a common ability in Magic: The Gathering, often seen on creatures, artifacts, or enchantments.\n",
            "Action: No need to use the query tool as the request is clear and straightforward.\n",
            "Final Answer:\n",
            "When this permanent enters the battlefield, draw two cards.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "-------FLAG = 0------\n",
            "[]\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: The user seems to be asking to search for a card that matches the previously described concept. I will use the query tool to find similar cards.\n",
            "\n",
            "Action: query\n",
            "Action Input: \"When this permanent enters the battlefield, draw two cards.\"\n",
            "\u001b[0mUtilizzando il dispositivo: cuda\n",
            "\n",
            "Observation: \u001b[36;1m\u001b[1;3m[{'image_url': 'https://cards.scryfall.io/border_crop/front/8/f/8fbb37d7-7053-4b1f-b899-91695f88f7e0.jpg?1674137796', 'distance': 0.3795604109764099}, {'image_url': 'https://cards.scryfall.io/border_crop/front/e/3/e3151960-cc0c-47b5-b476-295d7a17ae14.jpg?1738356865', 'distance': 0.4172326922416687}, {'image_url': 'https://cards.scryfall.io/border_crop/front/4/3/43ae8147-bf25-44f9-b75f-837b81ebe0de.jpg?1562785431', 'distance': 0.42349737882614136}, {'image_url': 'https://cards.scryfall.io/border_crop/front/e/7/e7796b82-5a7c-406e-be71-1f37abdbc44f.jpg?1598304433', 'distance': 0.4267352223396301}, {'image_url': 'https://cards.scryfall.io/border_crop/front/5/5/5595a57a-a76c-467b-afaf-5affffc24f35.jpg?1562915041', 'distance': 0.43301427364349365}, {'image_url': 'https://cards.scryfall.io/border_crop/front/3/4/3453084c-42cc-4241-b244-c79e704f96c8.jpg?1654568778', 'distance': 0.43470466136932373}, {'image_url': 'https://cards.scryfall.io/border_crop/front/a/3/a30c2025-03cf-4c3d-9dd5-0d25f7019d25.jpg?1690005447', 'distance': 0.4382669925689697}, {'image_url': 'https://cards.scryfall.io/border_crop/front/e/8/e86df81e-83d6-4522-82ad-2b1f2d8de623.jpg?1592672524', 'distance': 0.4438053369522095}, {'image_url': 'https://cards.scryfall.io/border_crop/front/5/9/5949bb9a-b4e8-4992-a12d-8e31953aff0d.jpg?1604195753', 'distance': 0.44509702920913696}, {'image_url': 'https://cards.scryfall.io/border_crop/front/d/e/defaeb68-3f8a-4740-b13f-8c71c7e9c8b4.jpg?1654568662', 'distance': 0.4472537040710449}, {'image_url': 'https://cards.scryfall.io/border_crop/front/8/4/84b5f6da-5e0f-4cce-92fe-7aa69124e265.jpg?1626096237', 'distance': 0.448322594165802}, {'image_url': 'https://cards.scryfall.io/border_crop/front/d/e/deea5690-6eb2-4353-b917-cbbf840e4e71.jpg?1730550969', 'distance': 0.4495493173599243}, {'image_url': 'https://cards.scryfall.io/border_crop/front/c/1/c198caf8-27ab-4300-841b-507e1b0ce9b3.jpg?1562803574', 'distance': 0.45302265882492065}, {'image_url': 'https://cards.scryfall.io/border_crop/front/2/b/2bf7258f-83c0-4cb0-8b1c-1f0c131abcd3.jpg?1562903970', 'distance': 0.4567314386367798}, {'image_url': 'https://cards.scryfall.io/border_crop/front/1/2/122e5e24-d704-4f1c-9577-0f25aaabb7ed.jpg?1562782685', 'distance': 0.45775866508483887}, {'image_url': 'https://cards.scryfall.io/border_crop/front/a/a/aa7514c5-27cd-46eb-ac85-4b1468a8064e.jpg?1562926836', 'distance': 0.4581063389778137}, {'image_url': 'https://cards.scryfall.io/border_crop/front/d/c/dc5ade73-12bc-46da-8dd4-3d4468858934.jpg?1562939299', 'distance': 0.45850956439971924}, {'image_url': 'https://cards.scryfall.io/border_crop/front/8/3/83be257c-8945-46be-8b58-fb2881084026.jpg?1562054974', 'distance': 0.4588223695755005}, {'image_url': 'https://cards.scryfall.io/border_crop/front/2/5/2593a6a6-dc21-4742-acb8-f7092931b1ce.jpg?1562903864', 'distance': 0.45966267585754395}, {'image_url': 'https://cards.scryfall.io/border_crop/front/0/4/04c0357a-e98d-4c49-83ad-d7a8ebe7e2d1.jpg?1562630982', 'distance': 0.4610562324523926}, {'image_url': 'https://cards.scryfall.io/border_crop/front/d/2/d2dcb8ed-23e7-4cee-9f43-042232c6035a.jpg?1562945240', 'distance': 0.46169954538345337}, {'image_url': 'https://cards.scryfall.io/border_crop/front/7/1/71cd91b2-0f9b-4582-ad90-32fa3ee1fde7.jpg?1572490265', 'distance': 0.4624224901199341}, {'image_url': 'https://cards.scryfall.io/border_crop/front/3/f/3fa57822-8bca-4cdd-916f-261dae494228.jpg?1583965783', 'distance': 0.46324944496154785}, {'image_url': 'https://cards.scryfall.io/border_crop/front/8/b/8b5f6b2b-8d98-4609-b991-a69523b5a07d.jpg?1562614198', 'distance': 0.4636174440383911}, {'image_url': 'https://cards.scryfall.io/border_crop/front/c/d/cdb3998e-b4a1-4dd1-8e12-24579ec8938b.jpg?1562799160', 'distance': 0.46443748474121094}, {'image_url': 'https://cards.scryfall.io/border_crop/front/1/0/100855e6-ac3e-481f-a462-afb5fcb5d476.jpg?1562432933', 'distance': 0.464860200881958}, {'image_url': 'https://cards.scryfall.io/border_crop/front/6/f/6faf4372-6fb5-48aa-9b94-b0e77c867116.jpg?1562878573', 'distance': 0.4650382399559021}, {'image_url': 'https://cards.scryfall.io/border_crop/front/8/5/857c2b6c-cfdf-4c88-a334-2937cb7db603.jpg?1562926442', 'distance': 0.4653545618057251}, {'image_url': 'https://cards.scryfall.io/border_crop/front/6/4/6402133e-eed1-4a46-9667-8b7a310362c1.jpg?1721426066', 'distance': 0.4668086767196655}, {'image_url': 'https://cards.scryfall.io/border_crop/front/0/1/01ea8333-38a8-4b7b-9f03-c5534df28353.jpg?1743206479', 'distance': 0.46765661239624023}]\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: When this permanent enters the battlefield, draw two cards.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "-----FLAG = 1-----\n",
            "[{'image_url': 'https://cards.scryfall.io/border_crop/front/8/f/8fbb37d7-7053-4b1f-b899-91695f88f7e0.jpg?1674137796', 'distance': 0.3795604109764099}, {'image_url': 'https://cards.scryfall.io/border_crop/front/e/3/e3151960-cc0c-47b5-b476-295d7a17ae14.jpg?1738356865', 'distance': 0.4172326922416687}, {'image_url': 'https://cards.scryfall.io/border_crop/front/4/3/43ae8147-bf25-44f9-b75f-837b81ebe0de.jpg?1562785431', 'distance': 0.42349737882614136}, {'image_url': 'https://cards.scryfall.io/border_crop/front/e/7/e7796b82-5a7c-406e-be71-1f37abdbc44f.jpg?1598304433', 'distance': 0.4267352223396301}, {'image_url': 'https://cards.scryfall.io/border_crop/front/5/5/5595a57a-a76c-467b-afaf-5affffc24f35.jpg?1562915041', 'distance': 0.43301427364349365}, {'image_url': 'https://cards.scryfall.io/border_crop/front/3/4/3453084c-42cc-4241-b244-c79e704f96c8.jpg?1654568778', 'distance': 0.43470466136932373}, {'image_url': 'https://cards.scryfall.io/border_crop/front/a/3/a30c2025-03cf-4c3d-9dd5-0d25f7019d25.jpg?1690005447', 'distance': 0.4382669925689697}, {'image_url': 'https://cards.scryfall.io/border_crop/front/e/8/e86df81e-83d6-4522-82ad-2b1f2d8de623.jpg?1592672524', 'distance': 0.4438053369522095}, {'image_url': 'https://cards.scryfall.io/border_crop/front/5/9/5949bb9a-b4e8-4992-a12d-8e31953aff0d.jpg?1604195753', 'distance': 0.44509702920913696}, {'image_url': 'https://cards.scryfall.io/border_crop/front/d/e/defaeb68-3f8a-4740-b13f-8c71c7e9c8b4.jpg?1654568662', 'distance': 0.4472537040710449}, {'image_url': 'https://cards.scryfall.io/border_crop/front/8/4/84b5f6da-5e0f-4cce-92fe-7aa69124e265.jpg?1626096237', 'distance': 0.448322594165802}, {'image_url': 'https://cards.scryfall.io/border_crop/front/d/e/deea5690-6eb2-4353-b917-cbbf840e4e71.jpg?1730550969', 'distance': 0.4495493173599243}, {'image_url': 'https://cards.scryfall.io/border_crop/front/c/1/c198caf8-27ab-4300-841b-507e1b0ce9b3.jpg?1562803574', 'distance': 0.45302265882492065}, {'image_url': 'https://cards.scryfall.io/border_crop/front/2/b/2bf7258f-83c0-4cb0-8b1c-1f0c131abcd3.jpg?1562903970', 'distance': 0.4567314386367798}, {'image_url': 'https://cards.scryfall.io/border_crop/front/1/2/122e5e24-d704-4f1c-9577-0f25aaabb7ed.jpg?1562782685', 'distance': 0.45775866508483887}, {'image_url': 'https://cards.scryfall.io/border_crop/front/a/a/aa7514c5-27cd-46eb-ac85-4b1468a8064e.jpg?1562926836', 'distance': 0.4581063389778137}, {'image_url': 'https://cards.scryfall.io/border_crop/front/d/c/dc5ade73-12bc-46da-8dd4-3d4468858934.jpg?1562939299', 'distance': 0.45850956439971924}, {'image_url': 'https://cards.scryfall.io/border_crop/front/8/3/83be257c-8945-46be-8b58-fb2881084026.jpg?1562054974', 'distance': 0.4588223695755005}, {'image_url': 'https://cards.scryfall.io/border_crop/front/2/5/2593a6a6-dc21-4742-acb8-f7092931b1ce.jpg?1562903864', 'distance': 0.45966267585754395}, {'image_url': 'https://cards.scryfall.io/border_crop/front/0/4/04c0357a-e98d-4c49-83ad-d7a8ebe7e2d1.jpg?1562630982', 'distance': 0.4610562324523926}, {'image_url': 'https://cards.scryfall.io/border_crop/front/d/2/d2dcb8ed-23e7-4cee-9f43-042232c6035a.jpg?1562945240', 'distance': 0.46169954538345337}, {'image_url': 'https://cards.scryfall.io/border_crop/front/7/1/71cd91b2-0f9b-4582-ad90-32fa3ee1fde7.jpg?1572490265', 'distance': 0.4624224901199341}, {'image_url': 'https://cards.scryfall.io/border_crop/front/3/f/3fa57822-8bca-4cdd-916f-261dae494228.jpg?1583965783', 'distance': 0.46324944496154785}, {'image_url': 'https://cards.scryfall.io/border_crop/front/8/b/8b5f6b2b-8d98-4609-b991-a69523b5a07d.jpg?1562614198', 'distance': 0.4636174440383911}, {'image_url': 'https://cards.scryfall.io/border_crop/front/c/d/cdb3998e-b4a1-4dd1-8e12-24579ec8938b.jpg?1562799160', 'distance': 0.46443748474121094}, {'image_url': 'https://cards.scryfall.io/border_crop/front/1/0/100855e6-ac3e-481f-a462-afb5fcb5d476.jpg?1562432933', 'distance': 0.464860200881958}, {'image_url': 'https://cards.scryfall.io/border_crop/front/6/f/6faf4372-6fb5-48aa-9b94-b0e77c867116.jpg?1562878573', 'distance': 0.4650382399559021}, {'image_url': 'https://cards.scryfall.io/border_crop/front/8/5/857c2b6c-cfdf-4c88-a334-2937cb7db603.jpg?1562926442', 'distance': 0.4653545618057251}, {'image_url': 'https://cards.scryfall.io/border_crop/front/6/4/6402133e-eed1-4a46-9667-8b7a310362c1.jpg?1721426066', 'distance': 0.4668086767196655}, {'image_url': 'https://cards.scryfall.io/border_crop/front/0/1/01ea8333-38a8-4b7b-9f03-c5534df28353.jpg?1743206479', 'distance': 0.46765661239624023}]\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "import UI_gradio\n",
        "\n",
        "prova=UI_gradio.start_UI().launch(share=True, debug = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X--wf-aP18kG",
        "outputId": "bda399a2-66c4-4257-d01b-1c104b7b0b23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EImYHxh-ZzdT"
      },
      "source": [
        "#Agent AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5_QkM0SZy4x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "dd512233-6de7-4e9f-8a3a-4ab25d12b6ca"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-23cc07ed737e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MISTRAL_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m llm = ChatOpenAI(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain.agents import initialize_agent, AgentType, Tool\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "import RAG_Connection as conn\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
        "\n",
        "user_input = input(\"\")\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"mistral-medium\",\n",
        "    base_url=\"https://api.mistral.ai/v1\",\n",
        "    api_key=api_key,\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "def query_empty_where(oracle_text : str):\n",
        "    \"\"\"Funzione che esegue una query su un database VDB\"\"\"\n",
        "    #Connessione al VDB\n",
        "    collection = conn.connect_to_vdb()\n",
        "    result=collection.query(\n",
        "        query_texts=[oracle_text], #testo da inserire e verrà embeddato a runtime\n",
        "        n_results=30#numero d carte che si vogliono\n",
        "    )\n",
        "    return result\n",
        "\n",
        "def translate(text: str) -> str:\n",
        "    response = llm.invoke(\"Translate the following text to English: \" + text)\n",
        "    return response.content\n",
        "\n",
        "def query(text: str):\n",
        "  return query_empty_where(translate(text))\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"query\",\n",
        "        func=query,\n",
        "        description=\"Use this tool to retrive the cards with the chosen text\"\n",
        "    )\n",
        "]\n",
        "\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "result = agent.run(user_input)\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ss8alZF2SVkD",
        "rNuZVn2CSuTh",
        "ZRlgQtyzTcXN",
        "EnWEeh12sxFB",
        "EImYHxh-ZzdT"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}